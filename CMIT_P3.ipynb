{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwTsgcuTdwwHcIgRnO+EPN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atloveu1226/CMIT2024/blob/main/CMIT_P3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Vw8E58RVwsHH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from scipy.optimize import linprog\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We study the case when we set the restriction on horizontal and vertical elements. The below are some preparition:\n"
      ],
      "metadata": {
        "id": "hcHirskHQ7EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b   = [3, 2, 3, 3, 2, 2, 1, 0, 1, 1, 3, 2, 1, 0, 0]\n",
        "\n",
        "Aeq = np.array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
        "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
        "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
        "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
        "       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
        "       [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
        "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]])\n",
        "\n",
        "beq = np.array(b[:8])\n",
        "\n",
        "c = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "\n",
        "\n",
        "lb = np.zeros(15)\n",
        "\n",
        "ub = np.ones(15)\n"
      ],
      "metadata": {
        "id": "5x29cXBfQ7l2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = linprog(\n",
        "    c,\n",
        "    A_eq = Aeq,\n",
        "    b_eq = beq,\n",
        "    bounds = list(zip(lb, ub)),\n",
        "    method = 'highs',\n",
        "   )\n",
        "print(result.x)\n",
        "\n",
        "#x = [1, 1, 0, 0, 0, 1, 0, 1, 0, 0,1, 1, 1, 0, 0]\n",
        "\n",
        "#print(np.dot(Aeq,x) == b[:8])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZhMzqGmU_V1",
        "outputId": "c2363c0f-e809-412f-80ef-e2a680d76e75"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.  0.  1.  1.  0.  1.  1. -0.  0.  0.  1.  1.  1. -0.  0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we shall define **7** different states, according the diagonal sum, namely, we define the 'STATE' function."
      ],
      "metadata": {
        "id": "t93REoRs30Ap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def STATE(x :np.array):\n",
        "\n",
        "  Dia_ob = b[8:15]\n",
        "\n",
        "  state = np.zeros(len(Dia_ob))\n",
        "\n",
        "  count = 0\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  #Translate diagonal restriction into matrix\n",
        "\n",
        "  M = np.array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "       [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
        "       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
        "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
        "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
        "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
        "\n",
        "  # Get the temporary variable\n",
        "\n",
        "  tem = np.dot(M, x)\n",
        "\n",
        "  for i in range(len(Dia_ob)):\n",
        "\n",
        "    if tem[i] == Dia_ob[i]:\n",
        "\n",
        "      count = count + 1\n",
        "\n",
        "  state[count - 1] = 1\n",
        "\n",
        "  return state"
      ],
      "metadata": {
        "id": "gQn9oSSY3zNl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, it is necessary to define the step function (i.e. from one state to next state)"
      ],
      "metadata": {
        "id": "EXoahCpxvG9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example to test the ability of this function\n",
        "\n",
        "#x_0 = np.array([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n",
        "\n",
        "#x_1 = np.array([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0])\n",
        "\n",
        "#print(STATE(x_0))\n",
        "\n",
        "#print(STATE(x_1))"
      ],
      "metadata": {
        "id": "M_qeXPHo7bUp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def OBJECT(c : np.array, Aeq, beq):\n",
        "\n",
        "    new_c = np.zeros(len(c))\n",
        "\n",
        "    result = linprog(\n",
        "    c,\n",
        "    A_eq = Aeq,\n",
        "    b_eq = beq,\n",
        "    bounds = list(zip(lb, ub)),\n",
        "    method = 'highs-ds',\n",
        "   )\n",
        "\n",
        "    solution = result.x\n",
        "\n",
        "    mean = np.mean(solution)\n",
        "\n",
        "    for i in range(len(solution)):\n",
        "\n",
        "      new_c[i - 1] = (solution[i - 1]- mean) ** 2\n",
        "\n",
        "    return [result, new_c]\n",
        "\n",
        "#solution: OBJECT(c)[0]\n",
        "#new_c : OBJECT(c)[1]\n",
        "\n"
      ],
      "metadata": {
        "id": "zjsl7pEAsNCW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(OBJECT(c, Aeq, beq)[0])\n",
        "print(Aeq.shape, beq.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxB5nChueOKd",
        "outputId": "1662548c-03d4-45c9-d81b-b76cbc9df7da"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 15) (8,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs_size = 7 #7\n",
        "n_actions = 7  #it has 7 different kind of actions\n",
        "HIDDEN_SIZE = 120\n",
        "\n",
        "net= nn.Sequential(\n",
        "            nn.Linear(obs_size, HIDDEN_SIZE),\n",
        "            nn.ReLU(),\n",
        "            #nn.Sigmoid(),\n",
        "            nn.Linear(HIDDEN_SIZE, n_actions)\n",
        "        )\n",
        "\n",
        "objective = nn.CrossEntropyLoss() #quite standard for classification tasks\n",
        "optimizer = optim.Adam(params=net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "vl6jREo8QwVR"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sm = nn.Softmax(dim=1) #Softmax converts the 7-dimensional output vector to a probability distribution\n",
        "\n",
        "def select_action(c, Aeq, beq):\n",
        "\n",
        "        x = OBJECT(c, Aeq, beq)[0].x\n",
        "\n",
        "        state = STATE(x)\n",
        "\n",
        "        state_t = torch.FloatTensor([state])\n",
        "\n",
        "        act_probs_t = sm(net(state_t))\n",
        "\n",
        "        #print(act_probs_t)\n",
        "\n",
        "        act_probs = act_probs_t.data.numpy()[0]\n",
        "\n",
        "        action = np.random.choice(len(act_probs), p=act_probs) #chooses randomly one of the 4 actions according to the probabilities returned by the net\n",
        "\n",
        "        return action\n"
      ],
      "metadata": {
        "id": "a2Pe7ivVJAtF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Explaination for above method\n",
        "\n",
        "#print(type(select_action(c, Aeq, beq)))"
      ],
      "metadata": {
        "id": "uWirZMGcP5di"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, it is necessary to define the STEP function:"
      ],
      "metadata": {
        "id": "Gc4ysH4e9KhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def next_state(c, next_action, Aeq, beq):\n",
        "\n",
        "  reward = 0\n",
        "\n",
        "  flag = False\n",
        "\n",
        "  current_state = 0\n",
        "\n",
        "  nextstage_ = 0\n",
        "\n",
        "  x = OBJECT(c, Aeq, beq)[0].x\n",
        "\n",
        "  state = STATE(x)\n",
        "\n",
        "  result = np.zeros(len(state))\n",
        "\n",
        "  for i in range(len(state)):\n",
        "\n",
        "    if state[i] == 1:\n",
        "\n",
        "      current_state = i\n",
        "\n",
        "      break\n",
        "\n",
        "  next_state_int = current_state + next_action\n",
        "\n",
        "  if OBJECT(c, Aeq, beq)[0].success == True:\n",
        "\n",
        "    if next_state_int <= len(state) - 1:\n",
        "\n",
        "      result[next_state_int] = 1\n",
        "\n",
        "    else:\n",
        "\n",
        "      result[(next_state_int + 2) % 7] = 1\n",
        "\n",
        "  else:\n",
        "\n",
        "    result = state\n",
        "\n",
        "  for j in range(len(state)):\n",
        "\n",
        "    if result[j] == 1:\n",
        "\n",
        "      nextstage_ = j\n",
        "\n",
        "      break\n",
        "\n",
        "  if nextstage_ == 6:\n",
        "\n",
        "    reward = 1\n",
        "\n",
        "    flag = True\n",
        "\n",
        "  elif nextstage_ < current_state:\n",
        "\n",
        "    reward = 0\n",
        "\n",
        "    flag = True\n",
        "\n",
        "  else:\n",
        "\n",
        "    reward = 0\n",
        "\n",
        "    flag = False\n",
        "\n",
        "  #print(current_state)\n",
        "\n",
        "\n",
        "  return result, reward, flag, nextstage_ + 1\n"
      ],
      "metadata": {
        "id": "HheOFtgx9Rw0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "\n",
        "  next_action = select_action(c, Aeq, beq)\n",
        "\n",
        "  print(next_action)\n",
        "\n",
        "  print(next_state(c, next_action, Aeq, beq))\n",
        "\n",
        "print(type(next_state(c, next_action, Aeq, beq)))"
      ],
      "metadata": {
        "id": "JEwaquUj9JXE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "fbad78fd-cf0e-44a9-cd5f-464b81f6b3c2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "(array([0., 0., 0., 0., 0., 0., 1.]), 1, True, 7)\n",
            "2\n",
            "(array([0., 0., 0., 1., 0., 0., 0.]), 0, True, 4)\n",
            "6\n",
            "(array([1., 0., 0., 0., 0., 0., 0.]), 0, True, 1)\n",
            "6\n",
            "(array([1., 0., 0., 0., 0., 0., 0.]), 0, True, 1)\n",
            "6\n",
            "(array([1., 0., 0., 0., 0., 0., 0.]), 0, True, 1)\n",
            "4\n",
            "(array([0., 0., 0., 0., 0., 1., 0.]), 0, True, 6)\n",
            "5\n",
            "(array([0., 0., 0., 0., 0., 0., 1.]), 1, True, 7)\n",
            "5\n",
            "(array([0., 0., 0., 0., 0., 0., 1.]), 1, True, 7)\n",
            "4\n",
            "(array([0., 0., 0., 0., 0., 1., 0.]), 0, True, 6)\n",
            "0\n",
            "(array([0., 0., 0., 0., 0., 0., 1.]), 1, True, 7)\n",
            "<class 'tuple'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-1f004a66f62a>:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  state_t = torch.FloatTensor([state])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we need to generate new restrction matrix from new state."
      ],
      "metadata": {
        "id": "YkAkGYpipx8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Matrix_Generator(Aeq, beq, n):\n",
        "    result1 = []\n",
        "\n",
        "    result2 = []\n",
        "\n",
        "    random_condition = []\n",
        "\n",
        "    Store1 = np.array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                      [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                      [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
        "                      [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
        "                      [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
        "                      [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
        "                      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
        "\n",
        "    Store2   = b[8:]\n",
        "\n",
        "    index = [9, 10, 11, 12, 13, 14, 15]\n",
        "\n",
        "    random_index = np.sort(np.random.choice(index, n, replace=False))\n",
        "\n",
        "    #print(random_index)\n",
        "\n",
        "    for i in range(n):\n",
        "\n",
        "        result1.append(Store1[random_index[i] - 9])\n",
        "\n",
        "        result2.append(Store2[random_index[i] - 9])\n",
        "\n",
        "    result1 = np.array(result1)\n",
        "\n",
        "    result2 = np.array(result2)\n",
        "\n",
        "    new_Aeq = np.concatenate((Aeq, result1), axis = 0)\n",
        "\n",
        "    if len(new_Aeq) >= 15:\n",
        "\n",
        "      new_Aeq = np.array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
        "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
        "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
        "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
        "       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
        "       [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
        "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
        "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "       [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
        "       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
        "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
        "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
        "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
        "\n",
        "    new_beq = np.concatenate((beq, result2), axis = 0)\n",
        "\n",
        "    if len(new_beq) >= 15:\n",
        "\n",
        "      new_beq = b\n",
        "\n",
        "\n",
        "    return [new_Aeq, new_beq]"
      ],
      "metadata": {
        "id": "zN9nQAoEpw_Q"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new = Matrix_Generator(Aeq, beq, 7)\n",
        "\n",
        "print(new[0], new[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrGhqPoGGxTf",
        "outputId": "89e11955-12ca-4555-91bb-7ece56f68ca2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 1 1 1 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1]\n",
            " [1 0 0 0 0 1 0 0 0 0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            " [0 0 1 0 0 0 0 1 0 0 0 0 1 0 0]\n",
            " [0 0 0 1 0 0 0 0 1 0 0 0 0 1 0]\n",
            " [0 0 0 0 1 0 0 0 0 1 0 0 0 0 1]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 1 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 1 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 1 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]] [2, 2, 3, 3, 2, 2, 0, 0, 1, 2, 1, 2, 1, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 200 #200\n",
        "\n",
        "GAMMA = 0.9\n",
        "\n",
        "PERCENTILE = 30 #30\n",
        "REWARD_GOAL = 0.8\n",
        "\n",
        "from collections import namedtuple  #more readable tuples\n",
        "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
        "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
      ],
      "metadata": {
        "id": "8aFafYbGt5tg"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "iter_no = 0\n",
        "reward_mean = 0\n",
        "full_batch = []\n",
        "batch = []\n",
        "episode_steps = []\n",
        "episode_reward = 0.0\n",
        "x = OBJECT(c, Aeq, beq)[0].x\n",
        "state = STATE(x)\n",
        "\n",
        "while reward_mean < REWARD_GOAL:\n",
        "\n",
        "        action = select_action(c, Aeq, beq)\n",
        "        #next_state, reward, episode_is_done, _ , _= env.step(action)\n",
        "        nextstate_, reward, episode_is_done, current_state = next_state(c, action, Aeq, beq)\n",
        "\n",
        "        episode_steps.append(EpisodeStep(observation=nextstate_, action=action))\n",
        "        episode_reward += reward\n",
        "\n",
        "        #print(episode_steps, episode_is_done)\n",
        "        #print(episode_reward)\n",
        "\n",
        "        if episode_is_done: # Episode finished\n",
        "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
        "\n",
        "\n",
        "            #print(len(batch))\n",
        "\n",
        "            Aeq = np.array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
        "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
        "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
        "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
        "       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
        "       [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
        "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]])\n",
        "\n",
        "            beq = np.array(b[:8])\n",
        "\n",
        "            c = [0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1]\n",
        "\n",
        "            episode_steps = []\n",
        "            episode_reward = 0.0\n",
        "\n",
        "            if len(batch) == BATCH_SIZE: # New set of batches ready --> select \"elite\"\n",
        "\n",
        "                #print(\"Batch full\")\n",
        "                #print(batch)\n",
        "                #print(\"\\n\")\n",
        "                #input(\"Press Enter to continue...\")\n",
        "\n",
        "\n",
        "                reward_mean = float(np.mean(list(map(lambda s: s.reward, batch)))) #compute mean reward (lambda is inline function)\n",
        "                elite_candidates= batch\n",
        "                #elite_candidates= batch + full_batch\n",
        "                returnG = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), elite_candidates))\n",
        "                reward_bound = np.percentile(returnG, PERCENTILE) #lowest score that is greater than PERCENTILE% of scores in the data set\n",
        "                                                                  #Keep the highest 100-PERCENTILE %\n",
        "                #print(\"Batch finished\", returnG, reward_bound)\n",
        "                #input(\"Press Enter to continue...\")\n",
        "\n",
        "\n",
        "                train_obs = []\n",
        "                train_act = []\n",
        "                elite_batch = []\n",
        "\n",
        "                for example, discounted_reward in zip(elite_candidates, returnG):\n",
        "                        if discounted_reward > reward_bound:\n",
        "                        #if discounted_reward >= reward_bound:\n",
        "                              train_obs.extend(map(lambda step: step.observation, example.steps))\n",
        "                              train_act.extend(map(lambda step: step.action, example.steps))\n",
        "                              elite_batch.append(example)\n",
        "                full_batch=elite_batch\n",
        "                state=train_obs\n",
        "                acts=train_act\n",
        "\n",
        "                #print(state)\n",
        "                #print(acts)\n",
        "                #input(\"Press Enter to continue...\")\n",
        "\n",
        "                #Do the training\n",
        "                if len(full_batch) != 0 : # just in case empty during an iteration\n",
        "                  state_t = torch.FloatTensor(state) #batch of states: [[1.0,0,0,0,0,0,0,0,0,0],[1,...]]\n",
        "                  acts_t = torch.LongTensor(acts) # batch of actions: [0,2,3,1,..]\n",
        "\n",
        "                  #print(state_t)\n",
        "                  #print(acts_t)\n",
        "                  #input(\"Press Enter to continue...\")\n",
        "                  optimizer.zero_grad() #it is good practice to do this, initializing the gradient computations\n",
        "                  action_scores_t = net(state_t)\n",
        "\n",
        "                  #print(action_scores_t)\n",
        "                  #input(\"Press Enter to continue...\")\n",
        "\n",
        "                  loss_t = objective(action_scores_t, acts_t)\n",
        "                  loss_t.backward() #computes the gradients\n",
        "                  optimizer.step() #updates the weights according to the gradients\n",
        "                  print(\"%d: loss=%.3f, reward_mean=%.3f\" % (iter_no, loss_t.item(), reward_mean))\n",
        "                  iter_no += 1\n",
        "                batch = [] #empty the batch\n",
        "\n",
        "        #state = next_state\n",
        "\n",
        "        c = OBJECT(c, Aeq, beq)[1]\n",
        "\n",
        "        new = Matrix_Generator(Aeq, beq, current_state)\n",
        "\n",
        "        Aeq = new[0]\n",
        "\n",
        "        beq = new[1]\n",
        "\n",
        "        new_x = OBJECT(c, Aeq, beq)[0].x\n",
        "\n",
        "        state = STATE(new_x)\n",
        "\n",
        "Aeq = np.array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
        "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
        "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
        "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
        "       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
        "       [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
        "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]])\n",
        "\n",
        "beq = np.array(b[:8])\n",
        "\n",
        "c = [0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1]\n",
        "\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "UI1bE5MKuO8Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6948bf9-6bcc-4705-980b-1831d1da5ec9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: loss=1.889, reward_mean=0.260\n",
            "1: loss=1.865, reward_mean=0.280\n",
            "2: loss=1.836, reward_mean=0.330\n",
            "3: loss=1.810, reward_mean=0.325\n",
            "4: loss=1.783, reward_mean=0.375\n",
            "5: loss=1.766, reward_mean=0.370\n",
            "6: loss=1.741, reward_mean=0.335\n",
            "7: loss=1.721, reward_mean=0.390\n",
            "8: loss=1.692, reward_mean=0.365\n",
            "9: loss=1.673, reward_mean=0.400\n",
            "10: loss=1.643, reward_mean=0.380\n",
            "11: loss=1.623, reward_mean=0.355\n",
            "12: loss=1.600, reward_mean=0.450\n",
            "13: loss=1.580, reward_mean=0.475\n",
            "14: loss=1.554, reward_mean=0.425\n",
            "15: loss=1.535, reward_mean=0.390\n",
            "16: loss=1.517, reward_mean=0.420\n",
            "17: loss=1.498, reward_mean=0.470\n",
            "18: loss=1.476, reward_mean=0.400\n",
            "19: loss=1.452, reward_mean=0.440\n",
            "20: loss=1.436, reward_mean=0.495\n",
            "21: loss=1.413, reward_mean=0.525\n",
            "22: loss=1.392, reward_mean=0.470\n",
            "23: loss=1.373, reward_mean=0.495\n",
            "24: loss=1.360, reward_mean=0.510\n",
            "25: loss=1.338, reward_mean=0.575\n",
            "26: loss=1.325, reward_mean=0.545\n",
            "27: loss=1.306, reward_mean=0.545\n",
            "28: loss=1.284, reward_mean=0.580\n",
            "29: loss=1.265, reward_mean=0.565\n",
            "30: loss=1.250, reward_mean=0.550\n",
            "31: loss=1.240, reward_mean=0.615\n",
            "32: loss=1.217, reward_mean=0.615\n",
            "33: loss=1.203, reward_mean=0.540\n",
            "34: loss=1.186, reward_mean=0.625\n",
            "35: loss=1.167, reward_mean=0.565\n",
            "36: loss=1.155, reward_mean=0.645\n",
            "37: loss=1.138, reward_mean=0.595\n",
            "38: loss=1.128, reward_mean=0.620\n",
            "39: loss=1.117, reward_mean=0.625\n",
            "40: loss=1.101, reward_mean=0.620\n",
            "41: loss=1.086, reward_mean=0.620\n",
            "42: loss=1.075, reward_mean=0.685\n",
            "43: loss=1.061, reward_mean=0.690\n",
            "44: loss=1.046, reward_mean=0.625\n",
            "45: loss=1.041, reward_mean=0.700\n",
            "46: loss=1.028, reward_mean=0.700\n",
            "47: loss=1.018, reward_mean=0.690\n",
            "48: loss=1.000, reward_mean=0.700\n",
            "49: loss=0.998, reward_mean=0.655\n",
            "--- 116.91091966629028 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test part:"
      ],
      "metadata": {
        "id": "ynDGO6hhSe-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.shape(Aeq), np.shape(beq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA_eGdOCqfgY",
        "outputId": "a0139166-d969-41c3-95dd-56107638c67f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 15) (8,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b_test = np.array(b[:8])\n",
        "\n",
        "c_test = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "A_test = np.array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
        "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
        "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
        "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
        "       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
        "       [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
        "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]])\n",
        "\n",
        "done = 0\n",
        "\n",
        "counter = 0\n",
        "\n",
        "MAXCOUNTER=7\n",
        "\n",
        "x = OBJECT(c_test, A_test, b_test)[0].x\n",
        "\n",
        "state = STATE(x)\n",
        "\n",
        "while done != 1 and counter < MAXCOUNTER:\n",
        "\n",
        "\n",
        "  print(\"Step: \"+str(counter))\n",
        "  reshaped_x = np.reshape(x, (3, 5))\n",
        "  #print(\"Reshaped solution matrix:\")\n",
        "  print(reshaped_x)\n",
        "  state_t = torch.FloatTensor([state])\n",
        "  action_scores_t = net(state_t)\n",
        "  act_probs_t = sm(action_scores_t)\n",
        "  #print(act_probs_t)\n",
        "  act_probs = act_probs_t.data.numpy()[0]\n",
        "  proposed_action=np.argmax(act_probs)\n",
        "  c_test = OBJECT(c_test, A_test, b_test)[1]\n",
        "  new = Matrix_Generator(Aeq, b_test, current_state)\n",
        "  A_test = new[0]\n",
        "  b_test = new[1]\n",
        "  #print(c_test, A_test, b_test)\n",
        "  x = OBJECT(c_test, A_test, b_test)[0].x\n",
        "  nextstate_, reward, episode_is_done, current_state = next_state(c_test, proposed_action, A_test, b_test)\n",
        "  #state=next_state.tolist() #changes NumPy array to a Python list\n",
        "  counter+=1\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-g4XjnmSbjq",
        "outputId": "a9dd3265-bbb3-4e3b-eb28-dc14fd163756"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0\n",
            "[[ 1.  0.  1.  1.  0.]\n",
            " [ 1.  1. -0.  0.  0.]\n",
            " [ 1.  1.  1. -0.  0.]]\n",
            "Step: 1\n",
            "[[ 1. -0.  1.  1. -0.]\n",
            " [ 1.  1.  0.  0.  0.]\n",
            " [ 1.  1.  1.  0.  0.]]\n",
            "Step: 2\n",
            "[[ 1. -0.  1.  1. -0.]\n",
            " [ 1.  1.  0.  0.  0.]\n",
            " [ 1.  1.  1.  0.  0.]]\n",
            "Step: 3\n",
            "[[ 1. -0.  1.  1. -0.]\n",
            " [ 1.  1.  0.  0.  0.]\n",
            " [ 1.  1.  1.  0.  0.]]\n",
            "Step: 4\n",
            "[[ 1. -0.  1.  1. -0.]\n",
            " [ 1.  1.  0.  0.  0.]\n",
            " [ 1.  1.  1.  0.  0.]]\n",
            "Step: 5\n",
            "[[ 1. -0.  1.  1. -0.]\n",
            " [ 1.  1.  0.  0.  0.]\n",
            " [ 1.  1.  1.  0.  0.]]\n",
            "Step: 6\n",
            "[[ 1. -0.  1.  1. -0.]\n",
            " [ 1.  1.  0.  0.  0.]\n",
            " [ 1.  1.  1.  0.  0.]]\n"
          ]
        }
      ]
    }
  ]
}