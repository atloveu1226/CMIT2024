{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOx4qeoevwJA7mYFHs1sg5o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atloveu1226/CMIT2024/blob/main/CMIT_P3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Vw8E58RVwsHH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from scipy.optimize import linprog\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We study the case when we set the restriction on horizontal and vertical elements. The below are some preparition:\n"
      ],
      "metadata": {
        "id": "hcHirskHQ7EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b   = [3, 2, 3, 3, 2, 2, 1, 0, 1, 1, 3, 2, 1, 0, 0]\n",
        "\n",
        "Aeq = np.array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
        "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
        "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
        "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
        "       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
        "       [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
        "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]])\n",
        "\n",
        "beq = np.array(b[:8])\n",
        "\n",
        "c = np.zeros(15)\n",
        "\n",
        "c[0] = 1\n",
        "\n",
        "lb = np.zeros(15)\n",
        "\n",
        "ub = np.ones(15)\n"
      ],
      "metadata": {
        "id": "5x29cXBfQ7l2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we shall define **7** different states, according the diagonal sum, namely, we define the 'STATE' function."
      ],
      "metadata": {
        "id": "t93REoRs30Ap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def STATE(x :np.array):\n",
        "\n",
        "  Dia_ob = b[8:15]\n",
        "\n",
        "  state = np.zeros(len(Dia_ob))\n",
        "\n",
        "  count = 0\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  #Translate diagonal restriction into matrix\n",
        "\n",
        "  M = np.array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "       [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
        "       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
        "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
        "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
        "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
        "\n",
        "  # Get the temporary variable\n",
        "\n",
        "  tem = np.dot(M, x)\n",
        "\n",
        "  for i in range(len(Dia_ob)):\n",
        "\n",
        "    if tem[i] == Dia_ob[i]:\n",
        "\n",
        "      count = count + 1\n",
        "\n",
        "  state[count - 1] = 1\n",
        "\n",
        "  return state"
      ],
      "metadata": {
        "id": "gQn9oSSY3zNl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, it is necessary to define the step function (i.e. from one state to next state)"
      ],
      "metadata": {
        "id": "EXoahCpxvG9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example to test the ability of this function\n",
        "\n",
        "#x_0 = np.array([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0])\n",
        "\n",
        "#x_1 = np.array([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0])\n",
        "\n",
        "#print(STATE(x_0))\n",
        "\n",
        "#print(STATE(x_1))"
      ],
      "metadata": {
        "id": "M_qeXPHo7bUp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def OBJECT(c : np.array, Aeq, beq):\n",
        "\n",
        "    new_c = np.zeros(len(c))\n",
        "\n",
        "    result = linprog(\n",
        "    c,\n",
        "    A_eq = Aeq,\n",
        "    b_eq = beq,\n",
        "    bounds = list(zip(lb, ub)),\n",
        "    method = 'highs-ds',\n",
        "   )\n",
        "\n",
        "    solution = result.x\n",
        "\n",
        "    mean = np.mean(solution)\n",
        "\n",
        "    for i in range(len(solution)):\n",
        "\n",
        "      new_c[i - 1] = (solution[i - 1]- mean) ** 2\n",
        "\n",
        "    return [result, new_c]\n",
        "\n",
        "#solution: OBJECT(c)[0]\n",
        "#new_c : OBJECT(c)[1]\n",
        "\n"
      ],
      "metadata": {
        "id": "zjsl7pEAsNCW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the neural network for coefficient vector $ùêú = (c_{1}, c_{2}, ... , c_{15})$, indeed the variation of $ùêú$ will cause the variation of $ùê±$."
      ],
      "metadata": {
        "id": "2x1l_DirQxk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obs_size = 7 #7\n",
        "n_actions = 7  #it has 7 different kind of actions\n",
        "HIDDEN_SIZE = 120\n",
        "\n",
        "net= nn.Sequential(\n",
        "            nn.Linear(obs_size, HIDDEN_SIZE),\n",
        "            nn.ReLU(),\n",
        "            #nn.Sigmoid(),\n",
        "            nn.Linear(HIDDEN_SIZE, n_actions)\n",
        "        )\n",
        "\n",
        "objective = nn.CrossEntropyLoss() #quite standard for classification tasks\n",
        "optimizer = optim.Adam(params=net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "vl6jREo8QwVR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sm = nn.Softmax(dim=1) #Softmax converts the 7-dimensional output vector to a probability distribution\n",
        "\n",
        "def select_action(c):\n",
        "\n",
        "        x = OBJECT(c, Aeq, beq)[0].x\n",
        "\n",
        "        state = STATE(x)\n",
        "\n",
        "        state_t = torch.FloatTensor([state])\n",
        "\n",
        "        act_probs_t = sm(net(state_t))\n",
        "\n",
        "        #print(act_probs_t)\n",
        "\n",
        "        act_probs = act_probs_t.data.numpy()[0]\n",
        "\n",
        "        action = np.random.choice(len(act_probs), p=act_probs) #chooses randomly one of the 4 actions according to the probabilities returned by the net\n",
        "\n",
        "        return action\n"
      ],
      "metadata": {
        "id": "a2Pe7ivVJAtF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Explaination for above method\n",
        "\n",
        "print(type(select_action(c)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWirZMGcP5di",
        "outputId": "b014b0b7-bc47-481f-ef18-8471025243c9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'int'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-2d5e8dcee0bc>:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  state_t = torch.FloatTensor([state])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, it is necessary to define the STEP function:"
      ],
      "metadata": {
        "id": "Gc4ysH4e9KhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def next_state(c, next_action, Aeq, beq):\n",
        "\n",
        "  x = OBJECT(c, Aeq, beq)[0].x\n",
        "\n",
        "  state = STATE(x)\n",
        "\n",
        "  result = np.zeros(len(state))\n",
        "\n",
        "  for current_state in range(len(state)):\n",
        "\n",
        "    if state[current_state] == 1:\n",
        "\n",
        "      break\n",
        "\n",
        "  next_state_int = current_state + next_action\n",
        "\n",
        "  if OBJECT(c, Aeq, beq)[0].success == True:\n",
        "\n",
        "    if next_state_int <= len(state) - 1:\n",
        "\n",
        "      result[next_state_int] = 1\n",
        "\n",
        "    else:\n",
        "\n",
        "      result[(next_state_int + 2) % 7] = 1\n",
        "\n",
        "  else:\n",
        "\n",
        "    result = state\n",
        "\n",
        "  return result\n",
        ""
      ],
      "metadata": {
        "id": "HheOFtgx9Rw0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#verify the above method\n",
        "print(OBJECT(c, Aeq, beq)[0].x)\n",
        "\n",
        "for i in range(10):\n",
        "\n",
        "  next_action = select_action(c)\n",
        "\n",
        "  print(next_action)\n",
        "\n",
        "  print(next_state(c, next_action, Aeq, beq))"
      ],
      "metadata": {
        "id": "JEwaquUj9JXE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "5b3300d4-2a2e-4d58-d82a-dec0df471e4f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.  0.  1.  1.  0.  1.  1. -0.  0.  0.  1.  1.  1. -0.  0.]\n",
            "4\n",
            "[0. 0. 0. 0. 0. 1. 0.]\n",
            "0\n",
            "[0. 0. 0. 0. 0. 0. 1.]\n",
            "4\n",
            "[0. 0. 0. 0. 0. 1. 0.]\n",
            "5\n",
            "[0. 0. 0. 0. 0. 0. 1.]\n",
            "2\n",
            "[0. 0. 0. 1. 0. 0. 0.]\n",
            "4\n",
            "[0. 0. 0. 0. 0. 1. 0.]\n",
            "5\n",
            "[0. 0. 0. 0. 0. 0. 1.]\n",
            "6\n",
            "[1. 0. 0. 0. 0. 0. 0.]\n",
            "3\n",
            "[0. 0. 0. 0. 1. 0. 0.]\n",
            "3\n",
            "[0. 0. 0. 0. 1. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we need to generate new restrction matrix from new state."
      ],
      "metadata": {
        "id": "YkAkGYpipx8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Matrix_Generator(Aeq, beq, n):\n",
        "    result1 = []\n",
        "\n",
        "    result2 = []\n",
        "\n",
        "    random_condition = []\n",
        "\n",
        "    Store1 = np.array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                      [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                      [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
        "                      [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
        "                      [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
        "                      [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
        "                      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
        "\n",
        "    Store2   = b[8:]\n",
        "\n",
        "    index = [9, 10, 11, 12, 13, 14, 15]\n",
        "\n",
        "    random_index = np.sort(np.random.choice(index, n, replace=False))\n",
        "\n",
        "    print(random_index)\n",
        "\n",
        "    for i in range(n):\n",
        "\n",
        "        result1.append(Store1[random_index[i] - 9])\n",
        "\n",
        "        result2.append(Store2[random_index[i] - 9])\n",
        "\n",
        "    result1 = np.array(result1)\n",
        "\n",
        "    result2 = np.array(result2)\n",
        "\n",
        "    new_Aeq = np.concatenate((Aeq, result1), axis = 0)\n",
        "\n",
        "    new_beq = np.concatenate((beq, result2), axis = 0)\n",
        "\n",
        "\n",
        "    return [new_Aeq, new_beq]"
      ],
      "metadata": {
        "id": "zN9nQAoEpw_Q"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new = Matrix_Generator(Aeq, beq, 2)\n",
        "\n",
        "print(new[0], new[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrGhqPoGGxTf",
        "outputId": "847e5184-d6e0-4e4b-928a-fbb6aa1e5b54"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 9 13]\n",
            "[[1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 1 1 1 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1]\n",
            " [1 0 0 0 0 1 0 0 0 0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0 1 0 0 0 0 1 0 0 0]\n",
            " [0 0 1 0 0 0 0 1 0 0 0 0 1 0 0]\n",
            " [0 0 0 1 0 0 0 0 1 0 0 0 0 1 0]\n",
            " [0 0 0 0 1 0 0 0 0 1 0 0 0 0 1]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 1 0 0 0 1 0 0]] [3 2 3 3 2 2 1 0 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100 #100\n",
        "\n",
        "GAMMA = 0.9\n",
        "\n",
        "PERCENTILE = 30 #30\n",
        "REWARD_GOAL = 0.8\n",
        "\n",
        "from collections import namedtuple  #more readable tuples\n",
        "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
        "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
      ],
      "metadata": {
        "id": "8aFafYbGt5tg"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "iter_no = 0\n",
        "reward_mean = 0\n",
        "full_batch = []\n",
        "batch = []\n",
        "episode_steps = []\n",
        "episode_reward = 0.0\n",
        "x = OBJECT(c).result\n",
        "state = STATE(x)\n",
        "\n",
        "while reward_mean < REWARD_GOAL:\n",
        "        action = select_action(state)\n",
        "        next_state, reward, episode_is_done, _ , _= env.step(action)\n",
        "\n",
        "        episode_steps.append(EpisodeStep(observation=state, action=action))\n",
        "        episode_reward += reward\n",
        "\n",
        "        #print(episode_steps)\n",
        "\n",
        "        if episode_is_done: # Episode finished\n",
        "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
        "\n",
        "\n",
        "            #print(len(batch))\n",
        "\n",
        "            next_state,_ = env.reset()\n",
        "            episode_steps = []\n",
        "            episode_reward = 0.0\n",
        "\n",
        "            if len(batch) == BATCH_SIZE: # New set of batches ready --> select \"elite\"\n",
        "\n",
        "                #print(\"Batch full\")\n",
        "                #print(batch)\n",
        "                #print(\"\\n\")\n",
        "                #input(\"Press Enter to continue...\")\n",
        "\n",
        "\n",
        "                reward_mean = float(np.mean(list(map(lambda s: s.reward, batch)))) #compute mean reward (lambda is inline function)\n",
        "                elite_candidates= batch\n",
        "                #elite_candidates= batch + full_batch\n",
        "                returnG = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), elite_candidates))\n",
        "                reward_bound = np.percentile(returnG, PERCENTILE) #lowest score that is greater than PERCENTILE% of scores in the data set\n",
        "                                                                  #Keep the highest 100-PERCENTILE %\n",
        "                #print(\"Batch finished\", returnG, reward_bound)\n",
        "                #input(\"Press Enter to continue...\")\n",
        "\n",
        "                train_obs = []\n",
        "                train_act = []\n",
        "                elite_batch = []\n",
        "\n",
        "                for example, discounted_reward in zip(elite_candidates, returnG):\n",
        "                        if discounted_reward > reward_bound:\n",
        "                        #if discounted_reward >= reward_bound:\n",
        "                              train_obs.extend(map(lambda step: step.observation, example.steps))\n",
        "                              train_act.extend(map(lambda step: step.action, example.steps))\n",
        "                              elite_batch.append(example)\n",
        "                full_batch=elite_batch\n",
        "                state=train_obs\n",
        "                acts=train_act\n",
        "\n",
        "                #print(state)\n",
        "                #print(acts)\n",
        "                #input(\"Press Enter to continue...\")\n",
        "\n",
        "                #Do the training\n",
        "                if len(full_batch) != 0 : # just in case empty during an iteration\n",
        "                  state_t = torch.FloatTensor(state) #batch of states: [[1.0,0,0,0,0,0,0,0,0,0],[1,...]]\n",
        "                  acts_t = torch.LongTensor(acts) # batch of actions: [0,2,3,1,..]\n",
        "\n",
        "                  #print(state_t)\n",
        "                  #print(acts_t)\n",
        "                  #input(\"Press Enter to continue...\")\n",
        "                  optimizer.zero_grad() #it is good practice to do this, initializing the gradient computations\n",
        "                  action_scores_t = net(state_t)\n",
        "\n",
        "                  #print(action_scores_t)\n",
        "                  #input(\"Press Enter to continue...\")\n",
        "\n",
        "                  loss_t = objective(action_scores_t, acts_t)\n",
        "                  loss_t.backward() #computes the gradients\n",
        "                  optimizer.step() #updates the weights according to the gradients\n",
        "                  print(\"%d: loss=%.3f, reward_mean=%.3f\" % (iter_no, loss_t.item(), reward_mean))\n",
        "                  iter_no += 1\n",
        "                batch = [] #empty the batch\n",
        "        state = next_state\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "UI1bE5MKuO8Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}