{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPJMHkAJ5LRW3Dx+V/nz8CW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atloveu1226/CMIT2024/blob/main/script2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some important packages that are necessary in this project:"
      ],
      "metadata": {
        "id": "OCUBwSTxcz0z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9WEWNljscp1w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from scipy.optimize import linprog\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With some parameters setting intially:"
      ],
      "metadata": {
        "id": "oC3ElC74dJUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b   = [3, 1, 2, 2, 2, 0, 1, 1, 0, 2, 1, 2, 1, 0, 0]\n",
        "\n",
        "Aeq = np.array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
        "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
        "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
        "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
        "       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
        "       [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
        "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]])\n",
        "\n",
        "beq = np.array(b[:8])\n",
        "\n",
        "c = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
        "\n",
        "\n",
        "\n",
        "lb = np.zeros(15)\n",
        "\n",
        "ub = np.ones(15)"
      ],
      "metadata": {
        "id": "4WatzsPmdBgL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a nerual network, which we feed a 15-dimensional vector and output a 15-dimensional vector."
      ],
      "metadata": {
        "id": "LMegV2lTdqOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obs_size = 15\n",
        "n_actions = 15  #it has 15 different kind of actions\n",
        "HIDDEN_SIZE = 120\n",
        "\n",
        "net= nn.Sequential(\n",
        "            nn.Linear(obs_size, HIDDEN_SIZE),\n",
        "            nn.ReLU(),\n",
        "            #nn.Sigmoid(),\n",
        "            nn.Linear(HIDDEN_SIZE, n_actions)\n",
        "        )\n",
        "\n",
        "objective = nn.CrossEntropyLoss() #quite standard for classification tasks\n",
        "optimizer = optim.Adam(params=net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "r6GD_ZE_dP7x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It also define a 'state' function, to numerically to compare the result when we change the coefficient vector $ùêú$. The result shows that whether the diagonal sum of corresponding binary matrix satisfies the given condition."
      ],
      "metadata": {
        "id": "h0OYcoVZeMaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def state(x):\n",
        "\n",
        "  Dia_ob = b[8:15]\n",
        "\n",
        "  #result = np.zeros(len(Dia_ob))\n",
        "\n",
        "  count = 0\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  #Translate diagonal restriction into matrix\n",
        "\n",
        "  M = np.array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "       [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
        "       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
        "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
        "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
        "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
        "\n",
        "  # Get the temporary variable\n",
        "\n",
        "  tem = np.dot(M, x)\n",
        "\n",
        "  for i in range(len(Dia_ob)):\n",
        "\n",
        "    if tem[i] == Dia_ob[i]:\n",
        "\n",
        "      count = count + 1\n",
        "\n",
        "  #state[count - 1] = 1\n",
        "\n",
        "  return count"
      ],
      "metadata": {
        "id": "AVfUC07OeLhS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(state(c))"
      ],
      "metadata": {
        "id": "fmY3DrJ3S7WQ",
        "outputId": "464472b6-a360-46fb-a9cb-5cae2c3e3e73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to define 'action' function, exerting action on $ùêú$."
      ],
      "metadata": {
        "id": "NiPLq_rZftjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sm = nn.Softmax(dim=1) #Softmax converts the 15-dimensional output vector to a probability distribution\n",
        "def select_action(c):\n",
        "\n",
        "  c_tensor = torch.FloatTensor([c])\n",
        "\n",
        "  act_probs_t = sm(net(c_tensor))\n",
        "\n",
        "  #print(act_probs_t)\n",
        "\n",
        "  act_probs = act_probs_t.data.numpy()[0]\n",
        "\n",
        "  action = np.random.choice(len(act_probs), p=act_probs) #chooses randomly one of the 4 actions according to the probabilities returned by the net\n",
        "\n",
        "  return action"
      ],
      "metadata": {
        "id": "8qeSHaJ0f6Ju"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example for the last function:"
      ],
      "metadata": {
        "id": "HCFxHDfCiO16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('action is', select_action(c))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvqsfWAnhecG",
        "outputId": "d3982dee-279d-40b3-8652-a1abc9ab2d5e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action is 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To simplify the code, we write a new function for 'linprog'."
      ],
      "metadata": {
        "id": "CU9kEumszhaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def object(c_vector):\n",
        "\n",
        "  result = linprog(\n",
        "    c_vector,\n",
        "    A_eq = Aeq,\n",
        "    b_eq = beq,\n",
        "    bounds = list(zip(lb, ub)),\n",
        "    method = 'highs',\n",
        "   )\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "k8fy_VlXzq01"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_sol = object(c).x\n",
        "\n",
        "print(state(current_sol))"
      ],
      "metadata": {
        "id": "kQt5ZL740Sh6",
        "outputId": "7a9011e7-628d-463a-ce19-4254895fdb12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then define the 'next_action' function:"
      ],
      "metadata": {
        "id": "FdCEcAkZicgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def next_step(c_vector, action):\n",
        "\n",
        "  flag = False\n",
        "\n",
        "  reward = 0\n",
        "\n",
        "  result = c_vector.copy()\n",
        "\n",
        "  #action = select_action(c)\n",
        "\n",
        "  #print(action)\n",
        "\n",
        "  #print(c)\n",
        "\n",
        "  result[action] = 1 - c_vector[action]\n",
        "\n",
        "  result = np.array(result)\n",
        "\n",
        "  #print(type(c_vector))\n",
        "\n",
        "  #print(type(result))\n",
        "\n",
        "  #para.x is an array, para.success return a bool value\n",
        "\n",
        "  para1 = object(c_vector)\n",
        "\n",
        "  para2 = object(result)\n",
        "\n",
        "  if para1.success == True and para2.success == True:\n",
        "\n",
        "    if state(para1.x) < state(para2.x):\n",
        "\n",
        "      reward = (state(para2.x)) / 7\n",
        "\n",
        "      flag = True\n",
        "\n",
        "    else:\n",
        "\n",
        "      reward = 0\n",
        "\n",
        "      flag = False\n",
        "\n",
        "  #if state(para2.x) == 7 or para2.success == False:\n",
        "\n",
        "    #flag = True\n",
        "\n",
        "  #else:\n",
        "\n",
        "    #flag = False\n",
        "\n",
        "\n",
        "\n",
        "  return [result, reward, flag]"
      ],
      "metadata": {
        "id": "SXHi4oODirIj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(c, state(c))\n",
        "\n",
        "#for i in range(10):\n",
        "\n",
        "  #action = select_action(c)\n",
        "\n",
        "  #result,_ ,_= next_step(c, action)\n",
        "\n",
        "  #print(next_step(c, action), action)\n",
        "\n",
        "  #print(state(result))\n",
        "\n",
        "#print(type(c))\n",
        "\n"
      ],
      "metadata": {
        "id": "0DKKsnwmjJqX",
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparation for training."
      ],
      "metadata": {
        "id": "2g1R1vck6QcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100 #100\n",
        "\n",
        "GAMMA = 0.9\n",
        "\n",
        "PERCENTILE = 30 #30\n",
        "REWARD_GOAL = 0.98\n",
        "\n",
        "from collections import namedtuple  #more readable tuples\n",
        "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
        "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
      ],
      "metadata": {
        "id": "1HXPKMhZkW8-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do the training process:"
      ],
      "metadata": {
        "id": "ke1XCLwz61KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "iter_no = 0\n",
        "reward_mean = 0\n",
        "full_batch = []\n",
        "batch = []\n",
        "episode_steps = []\n",
        "episode_reward = 0.0\n",
        "current_state = c.copy()\n",
        "\n",
        "while reward_mean < REWARD_GOAL:\n",
        "        action = select_action(current_state)\n",
        "        #print(type(current_state))\n",
        "        next_state = next_step(current_state, action)[0]\n",
        "        reward = next_step(current_state, action)[1]\n",
        "        episode_is_done = next_step(current_state, action)[2]\n",
        "        #print(next_step(current_state, action))\n",
        "\n",
        "        episode_steps.append(EpisodeStep(observation=current_state, action=action))\n",
        "        episode_reward += reward\n",
        "\n",
        "        #print(episode_steps)\n",
        "\n",
        "        if episode_is_done: # Episode finished\n",
        "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
        "\n",
        "\n",
        "            #print(len(batch))\n",
        "\n",
        "            next_state = c.copy()\n",
        "            episode_steps = []\n",
        "            episode_reward = 0.0\n",
        "\n",
        "            if len(batch) == BATCH_SIZE: # New set of batches ready --> select \"elite\"\n",
        "\n",
        "                #print(\"Batch full\")\n",
        "                #print(batch)\n",
        "                #print(\"\\n\")\n",
        "                #input(\"Press Enter to continue...\")\n",
        "\n",
        "\n",
        "                reward_mean = float(np.mean(list(map(lambda s: s.reward, batch)))) #compute mean reward (lambda is inline function)\n",
        "                #print(reward_mean)\n",
        "                elite_candidates= batch\n",
        "                #elite_candidates= batch + full_batch\n",
        "                returnG = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), elite_candidates))\n",
        "                reward_bound = np.percentile(returnG, PERCENTILE) #lowest score that is greater than PERCENTILE% of scores in the data set\n",
        "                                                                  #Keep the highest 100-PERCENTILE %\n",
        "                #print(\"Batch finished\", returnG, reward_bound)\n",
        "                #input(\"Press Enter to continue...\")\n",
        "\n",
        "                train_obs = []\n",
        "                train_act = []\n",
        "                elite_batch = []\n",
        "\n",
        "                for example, discounted_reward in zip(elite_candidates, returnG):\n",
        "                        if discounted_reward > reward_bound:\n",
        "                        #if discounted_reward >= reward_bound:\n",
        "                              train_obs.extend(map(lambda step: step.observation, example.steps))\n",
        "                              train_act.extend(map(lambda step: step.action, example.steps))\n",
        "                              elite_batch.append(example)\n",
        "                full_batch=elite_batch\n",
        "                current_state=train_obs\n",
        "                acts=train_act\n",
        "\n",
        "                #print(current_state)\n",
        "                #print(acts)\n",
        "                #input(\"Press Enter to continue...\")\n",
        "\n",
        "                #Do the training\n",
        "                if len(full_batch) != 0 : # just in case empty during an iteration\n",
        "                  state_t = torch.FloatTensor(current_state) #batch of states: [[1.0,0,0,0,0,0,0,0,0,0],[1,...]]\n",
        "                  acts_t = torch.LongTensor(acts) # batch of actions: [0,2,3,1,..]\n",
        "\n",
        "                  #print(state_t)\n",
        "                  #print(acts_t)\n",
        "                  #input(\"Press Enter to continue...\")\n",
        "                  optimizer.zero_grad() #it is good practice to do this, initializing the gradient computations\n",
        "                  action_scores_t = net(state_t)\n",
        "\n",
        "                  #print(action_scores_t)\n",
        "                  #input(\"Press Enter to continue...\")\n",
        "\n",
        "                  loss_t = objective(action_scores_t, acts_t)\n",
        "                  loss_t.backward() #computes the gradients\n",
        "                  optimizer.step() #updates the weights according to the gradients\n",
        "                  print(\"%d: loss=%.3f, reward_mean=%.3f\" % (iter_no, loss_t.item(), reward_mean))\n",
        "                  iter_no += 1\n",
        "                batch = [] #empty the batch\n",
        "            current_state = next_state\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "_zG-L2lZ-cw9",
        "outputId": "1d455d7a-a7a3-45b8-a468-6e130e113d5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: loss=2.689, reward_mean=0.949\n",
            "1: loss=2.664, reward_mean=0.891\n",
            "2: loss=2.612, reward_mean=0.926\n",
            "3: loss=2.591, reward_mean=0.960\n",
            "4: loss=2.613, reward_mean=0.926\n",
            "5: loss=2.660, reward_mean=0.891\n",
            "6: loss=2.623, reward_mean=0.960\n",
            "7: loss=2.595, reward_mean=0.937\n",
            "8: loss=2.536, reward_mean=0.949\n",
            "9: loss=2.503, reward_mean=0.960\n",
            "10: loss=2.526, reward_mean=0.909\n",
            "11: loss=2.551, reward_mean=0.960\n",
            "12: loss=2.502, reward_mean=0.949\n",
            "13: loss=2.423, reward_mean=0.937\n",
            "14: loss=2.471, reward_mean=0.971\n",
            "15: loss=2.342, reward_mean=0.943\n",
            "16: loss=2.435, reward_mean=0.954\n",
            "17: loss=2.391, reward_mean=0.949\n",
            "18: loss=2.366, reward_mean=0.943\n",
            "19: loss=2.358, reward_mean=0.966\n",
            "20: loss=2.345, reward_mean=0.954\n",
            "21: loss=2.094, reward_mean=0.983\n",
            "--- 35.24352669715881 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using network to choose a action and change the state."
      ],
      "metadata": {
        "id": "Yy3u0sn9ojRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "done=0\n",
        "counter=0\n",
        "MAXCOUNTER=2\n",
        "state_initial = c.copy()\n",
        "\n",
        "print(\"Step: \"+str(counter))\n",
        "print(state_initial)\n",
        "\n",
        "while done !=1 and counter<MAXCOUNTER:\n",
        "  state_t = torch.FloatTensor([state_initial])\n",
        "  action_scores_t = net(state_t)\n",
        "  act_probs_t = sm(action_scores_t)\n",
        "  #print(act_probs_t)\n",
        "  act_probs = act_probs_t.data.numpy()[0]\n",
        "  proposed_action=np.argmax(act_probs)\n",
        "  print(proposed_action)\n",
        "  next_state = next_step(state_initial, proposed_action)[0]\n",
        "  #reward = next_step(state_initial, proposed_action)[1]\n",
        "  #done = next_step(state_initial, proposed_action)[2]\n",
        "  #stat_e=next_state.tolist() #changes NumPy array to a Python list\n",
        "  counter+=1\n",
        "  print(\"Step: \"+str(counter))\n",
        "  print(next_state)\n",
        "  print((np.array(object(next_state).x)).reshape(3, 5))\n",
        "  state_initial = next_state\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Fc4vvjSFz2w",
        "outputId": "deab95fa-1ba1-4fbf-bc62-834677fd33d2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0\n",
            "[1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "14\n",
            "Step: 1\n",
            "[1 0 1 0 1 0 0 0 0 0 0 1 0 0 1]\n",
            "[[ 0.  1.  0.  1.  1.]\n",
            " [ 1. -0.  0.  0. -0.]\n",
            " [ 1.  1.  0.  0. -0.]]\n",
            "14\n",
            "Step: 2\n",
            "[1 0 1 0 1 0 0 0 0 0 0 1 0 0 0]\n",
            "[[ 1.  1.  0.  1. -0.]\n",
            " [-0.  1.  0.  0.  0.]\n",
            " [ 1. -0.  0.  0.  1.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-bdf6b15945e5>:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  state_t = torch.FloatTensor([state_initial])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print((object(c).x).reshape(3,5))"
      ],
      "metadata": {
        "id": "APQpdrD3JLBT"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}